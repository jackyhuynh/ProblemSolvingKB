{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before scrap go there and put linkedin/robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d1187b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Angela's Personal Site\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1 id=\"name\">\n",
      "   Angela Yu\n",
      "  </h1>\n",
      "  <p>\n",
      "   <em>\n",
      "    Founder of\n",
      "    <strong>\n",
      "     <a href=\"https://www.appbrewery.co/\">\n",
      "      The App Brewery\n",
      "     </a>\n",
      "    </strong>\n",
      "    .\n",
      "   </em>\n",
      "  </p>\n",
      "  <p>\n",
      "   I am an iOS and Web Developer. I ❤️ coffee and motorcycles.\n",
      "  </p>\n",
      "  <hr/>\n",
      "  <h3 class=\"heading\">\n",
      "   Books and Teaching\n",
      "  </h3>\n",
      "  <ul>\n",
      "   <li>\n",
      "    The Complete iOS App Development Bootcamp\n",
      "   </li>\n",
      "   <li>\n",
      "    The Complete Web Development Bootcamp\n",
      "   </li>\n",
      "   <li>\n",
      "    100 Days of Code - The Complete Python Bootcamp\n",
      "   </li>\n",
      "  </ul>\n",
      "  <hr/>\n",
      "  <h3 class=\"heading\">\n",
      "   Other Pages\n",
      "  </h3>\n",
      "  <a href=\"https://angelabauer.github.io/cv/hobbies.html\">\n",
      "   My Hobbies\n",
      "  </a>\n",
      "  <a href=\"https://angelabauer.github.io/cv/contact-me.html\">\n",
      "   Contact Me\n",
      "  </a>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# beautiful soup 4 for web scraping\n",
    "# import lxml\n",
    "\n",
    "with open(\"45_website.html\", encoding=\"utf8\") as file:\n",
    "    contents = file.read()\n",
    "\n",
    "soup = BeautifulSoup(contents, \"html.parser\")\n",
    "# html.parser or lxml may not work in some websites\n",
    "\n",
    "# print(soup.title)\n",
    "# print(soup.title.name)\n",
    "# print(soup.title.string)\n",
    "# print(soup)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dc187ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://www.appbrewery.co/\">The App Brewery</a>, <a href=\"https://angelabauer.github.io/cv/hobbies.html\">My Hobbies</a>, <a href=\"https://angelabauer.github.io/cv/contact-me.html\">Contact Me</a>]\n",
      "https://www.appbrewery.co/\n",
      "The App Brewery\n",
      "https://angelabauer.github.io/cv/hobbies.html\n",
      "My Hobbies\n",
      "https://angelabauer.github.io/cv/contact-me.html\n",
      "Contact Me\n",
      "<h3 class=\"heading\">Books and Teaching</h3>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find the info that we look for using Beautiful Soup\n",
    "\"\"\"\n",
    "\n",
    "all_anchor_tags = soup.find_all(name=\"a\")\n",
    "print(all_anchor_tags)\n",
    "for tag in all_anchor_tags:\n",
    "    print(tag.get(\"href\"))\n",
    "    print(tag.getText())\n",
    "\n",
    "heading = soup.find(name=\"h1\", id=\"name\")\n",
    "# print(heading)\n",
    "\n",
    "section_heading = soup.find(name=\"h3\", class_=\"heading\")\n",
    "print(section_heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "672e488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skio is hiring founding engineers ($150-300k, lots of equity) https://skio.com/careers\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get response from the website\n",
    "response = requests.get(\"https://news.ycombinator.com/\")\n",
    "# Store the whole html to the page\n",
    "yc_web_page = response.text\n",
    "\n",
    "# Create the soup to access\n",
    "soup = BeautifulSoup(yc_web_page, \"html.parser\")\n",
    "\n",
    "# find the frst anchor task, with class = title\n",
    "articles = soup.find_all(name=\"a\", class_=\"titlelink\")\n",
    "\n",
    "article_texts = []\n",
    "article_links = []\n",
    "# get the text inside the article_tag\n",
    "for article_tag in articles:\n",
    "    article_texts.append(article_tag.getText())\n",
    "    article_links.append(article_tag.get(\"href\"))\n",
    "\n",
    "article_upvotes = [int(score.getText().split()[0]) for score in soup.find_all(name=\"span\", class_=\"score\")]\n",
    "\n",
    "# print(article_upvotes)\n",
    "# Find the largest index by using the max and largest function\n",
    "largest_index = article_upvotes.index(max(article_upvotes))\n",
    "\n",
    "print(article_texts[largest_index], article_links[largest_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bad23a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1) The Godfather', '2) The Empire Strikes Back', '3) The Dark Knight', '4) The Shawshank Redemption', '5) Pulp Fiction', '6) Goodfellas', '7) Raiders Of The Lost Ark', '8) Jaws', '9) Star Wars', '10) The Lord Of The Rings: The Fellowship Of The Ring', '11) Back To The Future', '12: The Godfather Part II', '13) Blade Runner', '14) Alien', '15) Aliens', '16) The Lord Of The Rings: The Return Of The King', '17) Fight Club', '18) Inception', '19) Jurassic Park', '20) Die Hard', '21) 2001: A Space Odyssey', '22) Apocalypse Now', '23) The Lord Of The Rings: The Two Towers', '24) The Matrix', '25) Terminator 2: Judgment Day', '26) Heat', '27) The Good, The Bad And The Ugly', '28) Casablanca', '29) The Big Lebowski', '30) Seven', '31) Taxi Driver', '32) The Usual Suspects', \"33) Schindler's List\", '34) Guardians Of The Galaxy', '35) The Shining', '36) The Departed', '37) The Thing', '38) Mad Max: Fury Road', '39) Saving Private Ryan', '40) 12 Angry Men', '41) Eternal Sunshine Of The Spotless Mind', '42) There Will Be Blood', \"43) One Flew Over The Cuckoo's Nest\", '44) Gladiator', '45) Drive', '46) Citizen Kane', '47) Interstellar', '48) The Silence Of The Lambs', '49) Trainspotting', '50) Lawrence Of Arabia', \"51) It's A Wonderful Life\", '52) Once Upon A Time In The West', '53) Psycho', '54) Vertigo', \"55) Pan's Labyrinth\", '56) Reservoir Dogs', '57) Whiplash', '58) Inglourious Basterds', '59) E.T. – The Extra Terrestrial', '60) American Beauty', '61) Forrest Gump', '62) La La Land', '63) Donnie Darko', '64) L.A. Confidential', '65) Avengers Assemble', '66) Return Of The Jedi', '67) Memento', '68) Ghostbusters', \"69) Singin' In The Rain\", '70) The Lion King', '71) Hot Fuzz', '72) Rear Window', '73) Seven Samurai', '74) Mulholland Dr.', '75) Fargo', '76) A Clockwork Orange', '77) Toy Story', '78) Oldboy', '79) Captain America: Civil War', '15) Spirited Away', '81) The Social Network', '82) Some Like It Hot', '83) True Romance', '84) Rocky', '85) Léon', '86) Indiana Jones And The Last Crusade', '87) Predator', '88) The Exorcist', '89) Shaun Of The Dead', '90) No Country For Old Men', '91) The Prestige', '92) The Terminator', '93) The Princess Bride', '94) Lost In Translation', '95) Arrival', '96) Good Will Hunting', '97) Titanic', '98) Amelie', '99) Raging Bull', '100) Stand By Me']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL=\"https://web.archive.org/web/20200518073855/https://www.empireonline.com/movies/features/best-movies-2/\"\n",
    "\n",
    "response = requests.get(URL)\n",
    "web_site_html = response.text\n",
    "\n",
    "soup = BeautifulSoup(web_site_html, \"html.parser\")\n",
    "\n",
    "all_movies = soup.find_all(name=\"h3\", class_=\"title\")\n",
    "# print(all_movies)\n",
    "movie_titles = [movie.getText() for movie in all_movies]\n",
    "movies = movie_titles[::-1]\n",
    "\n",
    "with open(\"movies.txt\", mode=\"w\") as file:\n",
    "    for movie in movies:\n",
    "        file.write(f\"{movie}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
